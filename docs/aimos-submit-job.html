<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Submit jobs &mdash; AiMOS 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AI Benchmarks" href="aimos-benchmark.html" />
    <link rel="prev" title="Set up the workload environment" href="aimos-workload.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> AiMOS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="aimos-basics.html">AiMOS Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-userid.html">Access to AiMOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-env-basics.html">AiMOS Environment Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-workload.html">Set up the workload environment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Submit jobs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-job">Interactive Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batch-job">Batch job</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-the-job-dependencies">Build the job dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#request-for-up-to-48-hours-run-time">Request for up to 48 hours run time</a></li>
<li class="toctree-l2"><a class="reference internal" href="#request-for-nvme-storage-on-the-compute-nodes">Request for NVMe storage on the compute nodes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="aimos-benchmark.html">AI Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-ai-hardware-toolkit.html">Analog AI Hardware Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-hints-tips.html">Hints and Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="aimos-getting-help.html">Getting Help?</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AiMOS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Submit jobs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/aimos-submit-job.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="submit-jobs">
<h1>Submit jobs<a class="headerlink" href="#submit-jobs" title="Permalink to this headline"></a></h1>
<p>You use Slurm commands to submit jobs to AiMOS.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>You logged in to one of the front end nodes.  For how to see <a class="reference internal" href="aimos-userid.html#how-to-login"><span class="std std-ref">Login to AiMOS</span></a>.</p></li>
</ul>
<figure class="align-default">
<img alt="_images/aimos-overview.png" src="_images/aimos-overview.png" />
</figure>
<ul>
<li><p>You must specify “–gres=gpu:&lt;value&gt;” option with the <strong>salloc</strong> or <strong>sbatch</strong> command if you want to allocate the compute nodes from AiMOS.</p>
<blockquote>
<div><ul class="simple">
<li><p>The value is between 1 and 6 for DCS(Power) cluster.</p></li>
<li><p>The value is between  1 and 8 for NPL(X86) cluster.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>This the number of gpu that you want per node.  If you specify –gres=gpu:6 for DCS cluster or –gres=gpu:8 for NPL cluster, you are in essence would get the whole node(s) allocated to you.  For more information regarding GPU on AiMOS, see <a class="reference external" href="https://secure.cci.rpi.edu/wiki/clusters/DCS_Supercomputer/#using-gpus">https://secure.cci.rpi.edu/wiki/clusters/DCS_Supercomputer/#using-gpus</a>.</p></li>
<li><p>You also must specify the time required to run your job via option <strong>-t &lt;value&gt;</strong>.  The value is number of minutes. See <a class="reference internal" href="aimos-env-basics.html#job-queue"><span class="std std-ref">Job Queue Limits</span></a> for more information.</p></li>
</ul>
</section>
<section id="interactive-job">
<h2>Interactive Job<a class="headerlink" href="#interactive-job" title="Permalink to this headline"></a></h2>
<p>It is assumed that you already ssh to one of the front end nodes.  You use the <strong>salloc</strong> command to allocate 1 node with 6 gpu for 15 minutes. After the command returns, you now run squeue command to see which node is allocated for the interactive session. In the example below, dcs249 is allocated. Now you can ssh to the node and execute your application.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$  salloc -N <span class="m">1</span> --gres<span class="o">=</span>gpu:6 -t <span class="m">15</span>
salloc: Granted job allocation <span class="m">60780</span>
<span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
             <span class="m">60780</span>       dcs     bash your-id  R       <span class="m">1</span>:07      <span class="m">1</span> dcs249
<span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$ ssh dcs249
Warning: Permanently added <span class="s1">&#39;dcs249,172.31.236.249&#39;</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
<span class="o">[</span>your-id@dcs249 ~<span class="o">]</span>$ ls
barn  barn-shared  etc  Miniconda3-latest-Linux-ppc64le.sh  scratch  scratch-shared  var
<span class="o">[</span>your-id@dcs249 ~<span class="o">]</span>$ hostname -f
dcs249.ccni.rpi.edu
</pre></div>
</div>
<p>After the specified time, which is 15 minutes in this example, the node is deallocated and you will no longer be allowed to ssh to the node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>your-id@dcs249 ~<span class="o">]</span>$ salloc: Job <span class="m">60780</span> has exceeded its <span class="nb">time</span> limit and its allocation has been revoked.
   Killed by signal <span class="m">1</span>.
<span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$ ssh dcs249
Access denied: user yourid <span class="o">(</span><span class="nv">uid</span><span class="o">=</span><span class="m">6112</span><span class="o">)</span> has no active <span class="nb">jobs</span> on this node.
Access denied by pam_slurm_adopt: you have no active <span class="nb">jobs</span> on this node
Authentication failed.
</pre></div>
</div>
</section>
<section id="batch-job">
<h2>Batch job<a class="headerlink" href="#batch-job" title="Permalink to this headline"></a></h2>
<p>You use <strong>sbatch</strong> Slurm command to submit a batch job.  You can create a scriptthat contains a list of Slurm directives (or commands) to tell Slurm what to do. This is a sample script to run a hello_MPI_c program.</p>
<p><strong>PREREQUISITE:</strong> Passwordless is required for MPI job.  For how to see <a class="reference internal" href="aimos-env-basics.html#setup-environment"><span class="std std-ref">Initial Environment Setup</span></a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -x</span>

<span class="c1"># The lines started with SBATCH are directives to sbatch command.  Alternately, they can be specified on the command line.</span>
<span class="c1">#SBATCH -J hello_MPI</span>
<span class="c1">#SBATCH -o hello_MPI_%j.out</span>
<span class="c1">#SBATCH -e hello_MPI_%j.err</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#SBATCH --mail-user=&lt;you email address&gt;</span>
<span class="c1">#SBATCH --gres=gpu:6</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="c1">#SBATCH --time=02:00:00</span>


<span class="c1"># SLURM_NPROCS and SLURM_NTASK_PER_NODE env variables are set by sbatch Slurm commands based on the SBATCH directives above</span>
<span class="c1"># or options specified on the command line.</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;x</span><span class="nv">$SLURM_NPROCS</span><span class="s2">&quot;</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span> <span class="o">]</span>
<span class="k">then</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;x</span><span class="nv">$SLURM_NTASKS_PER_NODE</span><span class="s2">&quot;</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span> <span class="o">]</span>
  <span class="k">then</span>
    <span class="nv">SLURM_NTASKS_PER_NODE</span><span class="o">=</span><span class="m">1</span>
  <span class="k">fi</span>
  <span class="nv">SLURM_NPROCS</span><span class="o">=</span><span class="sb">`</span>expr <span class="nv">$SLURM_JOB_NUM_NODES</span> <span class="se">\*</span> <span class="nv">$SLURM_NTASKS_PER_NODE</span><span class="sb">`</span>
<span class="k">else</span>
  <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;x</span><span class="nv">$SLURM_NTASKS_PER_NODE</span><span class="s2">&quot;</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span> <span class="o">]</span>
  <span class="k">then</span>
    <span class="nv">SLURM_NTASKS_PER_NODE</span><span class="o">=</span><span class="sb">`</span>expr <span class="nv">$SLURM_NPROCS</span> / <span class="nv">$SLURM_JOB_NUM_NODES</span><span class="sb">`</span>
  <span class="k">fi</span>
<span class="k">fi</span>

<span class="c1"># Get the host name of the allocated compute node(s) and generate the host list file.</span>
srun hostname -s <span class="p">|</span> sort -u &gt; ~/tmp/hosts.<span class="nv">$SLURM_JOBID</span>
awk <span class="s2">&quot;{ print \$0 \&quot;-ib slots=</span><span class="nv">$SLURM_NTASKS_PER_NODE</span><span class="s2">\&quot;; }&quot;</span> ~/tmp/hosts.<span class="nv">$SLURM_JOBID</span> &gt;~/tmp/tmp.<span class="nv">$SLURM_JOBID</span>
mv ~/tmp/tmp.<span class="nv">$SLURM_JOBID</span> ~/tmp/hosts.<span class="nv">$SLURM_JOBID</span>

<span class="c1"># Load the required tools and libraries for the job.</span>
module load gcc/6.4.0/1
module load spectrum-mpi

<span class="c1"># Submit the job.</span>
mpirun --bind-to core --report-bindings -hostfile ~/tmp/hosts.<span class="nv">$SLURM_JOBID</span> -np <span class="nv">$SLURM_NPROCS</span> &lt;PATH&gt;/hello_MPI_c

<span class="c1"># Remove the generated host list file</span>
rm ~/tmp/hosts.<span class="nv">$SLURM_JOBID</span>
</pre></div>
</div>
<p>Submit the  above sample job via <strong>sbatch</strong> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch ./hello_MPI.sh
</pre></div>
</div>
<p>Note: that you can specify the command options on the <strong>sbatch</strong> command line instead of using #SBATCH directive like in the sample script above.</p>
<p>With #SBATCH –mail-type=ALL, #SBATCH –mail-user=&lt;you email address&gt;, you should receive the email from Slurm when a job starts and ends to your email address.</p>
<p>You should also see the &lt;job name&gt;_&lt;job_id&gt;.out and &lt;job name&gt;_&lt;job_id&gt;.err in your current directory with #SBATCH -o &lt;job name&gt;_%j.out and #SBATCH -e &lt;job name&gt;_%j.err after the job completes.</p>
</section>
<section id="build-the-job-dependencies">
<h2>Build the job dependencies<a class="headerlink" href="#build-the-job-dependencies" title="Permalink to this headline"></a></h2>
<p>You can use <code class="docutils literal notranslate"><span class="pre">--dependency</span></code> flag of sbatch to build a list of jobs to run in order.  For more information see  <a class="reference external" href="https://slurm.schedmd.com/sbatch.html">https://slurm.schedmd.com/sbatch.html</a>.</p>
<p>You can find examples for how to use this flag at the <a class="reference external" href="https://hpc.nih.gov/docs/job_dependencies.html#intro">link</a>.</p>
</section>
<section id="request-for-up-to-48-hours-run-time">
<h2>Request for up to 48 hours run time<a class="headerlink" href="#request-for-up-to-48-hours-run-time" title="Permalink to this headline"></a></h2>
<p>The default maximum time limit is 360 minutes  which is 6 hours. It is recommended that you include checkpoint restart in your code to enable your job to restart at the last checkpoint if your job run is longer than 6 hours. For information on how to implement the checkpoint in pytorch, you can refer to <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html</a></p>
<p>However, Both NPL and DCS clusters now have a capability for a 48 hour job time limit.  There are maximum 18 nodes that you can request for the 48 hour time limit in DCS cluster. There are maximum of 4 nodes that you can request for this option in NPL cluster. To request this capability, you must include the following line in your salloc or sbatch command.</p>
<p>On DCS cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--qos<span class="o">=</span>dcs-48hr
</pre></div>
</div>
<p>On NPL cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--qos<span class="o">=</span>npl-48hr
</pre></div>
</div>
<p>Or the following line in your batch script:</p>
<p>On DCS cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --qos=dcs-48hr</span>
</pre></div>
</div>
<p>On NPL cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --qos=npl-48hr</span>
</pre></div>
</div>
</section>
<section id="request-for-nvme-storage-on-the-compute-nodes">
<span id="request-nvme"></span><h2>Request for NVMe storage on the compute nodes<a class="headerlink" href="#request-for-nvme-storage-on-the-compute-nodes" title="Permalink to this headline"></a></h2>
<p><strong>NOTE:</strong> NVMe storage is only available on the DCS cluster.  It is not yet available on the NPL cluster.</p>
<p>To request NVMe storage, specify –gres=nvme with your Slurm commands. This can be combined with other requests, such as GPUs. When the first job step starts, the system will initialize the storage and create the path /mnt/nvme/uid_${SLURM_JOB_UID}/job_${SLURM_JOBID}.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$  salloc -N <span class="m">1</span> --gres<span class="o">=</span>gpu:6,nvme -t <span class="m">30</span>
salloc: Granted job allocation <span class="m">64444</span>
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
             <span class="m">64444</span>       dcs     bash BMHRkmkh  R       <span class="m">0</span>:11      <span class="m">1</span> dcs055
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcsfen01 ~<span class="o">]</span>$ ssh dcs055
Warning: Permanently added <span class="s1">&#39;dcs055,172.31.236.55&#39;</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcs055 ~<span class="o">]</span>$
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcs055 ~<span class="o">]</span>$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        243G     <span class="m">0</span>  243G   <span class="m">0</span>% /dev
tmpfs           256G   64K  256G   <span class="m">1</span>% /dev/shm
tmpfs           256G   25M  256G   <span class="m">1</span>% /run
tmpfs           256G     <span class="m">0</span>  256G   <span class="m">0</span>% /sys/fs/cgroup
rootfs          256G  <span class="m">7</span>.6G  249G   <span class="m">3</span>% /
rw              256G   64K  256G   <span class="m">1</span>% /.sllocal/log
gpfs.u          <span class="m">1</span>.1P  387T  640T  <span class="m">38</span>% /gpfs/u
/dev/nvme0n1    <span class="m">1</span>.5T   77M  <span class="m">1</span>.5T   <span class="m">1</span>% /mnt/nvme
<span class="o">(</span>base<span class="o">)</span> <span class="o">[</span>your-id@dcs055 job_64444<span class="o">]</span>$ <span class="nb">pwd</span>
/mnt/nvme/uid_6112/job_64444
</pre></div>
</div>
<p><strong>NOTE:</strong> The NVMe storage is not persistent between allocations.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="aimos-workload.html" class="btn btn-neutral float-left" title="Set up the workload environment" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="aimos-benchmark.html" class="btn btn-neutral float-right" title="AI Benchmarks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, AiMOS.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>